{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr39LF4tjtPY9DLv1NupW5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash007data/akashl-learning-git/blob/main/Assignment2_G24AI2040_AKASH_TIWARI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**link for colab**: https://colab.research.google.com/drive/1iMBUbLZvM3KxCMb7xM0nEPIeYIdk8qCk?usp=sharing"
      ],
      "metadata": {
        "id": "rck9FkPM-pdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the csv files directly from assignment link\n",
        "\n",
        "# cruise ships data\n",
        "!wget https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/cruise.csv\n",
        "\n",
        "# customer churn data\n",
        "!wget https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/customer_churn.csv\n",
        "\n",
        "# e-commerce customer data\n",
        "!wget https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/e-com_customer.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdeYeykgJZ2W",
        "outputId": "c4f03ce6-c19b-48c8-9df4-48186e4f72c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-29 10:09:58--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/cruise.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8734 (8.5K) [text/plain]\n",
            "Saving to: ‘cruise.csv’\n",
            "\n",
            "cruise.csv          100%[===================>]   8.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-29 10:09:59 (52.9 MB/s) - ‘cruise.csv’ saved [8734/8734]\n",
            "\n",
            "--2025-07-29 10:09:59--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/customer_churn.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 115479 (113K) [text/plain]\n",
            "Saving to: ‘customer_churn.csv’\n",
            "\n",
            "customer_churn.csv  100%[===================>] 112.77K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-07-29 10:09:59 (6.49 MB/s) - ‘customer_churn.csv’ saved [115479/115479]\n",
            "\n",
            "--2025-07-29 10:09:59--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/e-com_customer.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86871 (85K) [text/plain]\n",
            "Saving to: ‘e-com_customer.csv’\n",
            "\n",
            "e-com_customer.csv  100%[===================>]  84.83K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-07-29 10:09:59 (5.43 MB/s) - ‘e-com_customer.csv’ saved [86871/86871]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java (OpenJDK 11) for Hadoop\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install openjdk-11-jdk-headless -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inkPPnYYnRzQ",
        "outputId": "8ae1dd42-073b-49cf-f39f-a7bfc3e22846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-11-jdk-headless is already the newest version (11.0.28+6-1ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download Hadoop\n",
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8OHnOzEoDvW",
        "outputId": "788c796d-2d01-4a17-90bf-bc86f1505551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-29 10:03:04--  https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 730107476 (696M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.6.tar.gz’\n",
            "\n",
            "hadoop-3.3.6.tar.gz 100%[===================>] 696.28M  20.0MB/s    in 38s     \n",
            "\n",
            "2025-07-29 10:03:43 (18.2 MB/s) - ‘hadoop-3.3.6.tar.gz’ saved [730107476/730107476]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract and move Hadoop\n",
        "!tar -xzf hadoop-3.3.6.tar.gz\n",
        "!mv hadoop-3.3.6 /usr/local/hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whu-seleoYlh",
        "outputId": "ed3b1e23-38d6-4458-cac0-85337707a7fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot move 'hadoop-3.3.6' to '/usr/local/hadoop/hadoop-3.3.6': Directory not empty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the previous Hadoop folder\n",
        "!rm -rf /usr/local/hadoop\n",
        "\n",
        "# Move freshly extracted Hadoop to correct location\n",
        "!mv hadoop-3.3.6 /usr/local/hadoop\n"
      ],
      "metadata": {
        "id": "aDHIS5HhpjEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables for Hadoop\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"PATH\"] += os.pathsep + \"/usr/local/hadoop/bin\"\n"
      ],
      "metadata": {
        "id": "RTsLzZ3KpqZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKDAI3iQprvz",
        "outputId": "4e7214e8-6292-4dfb-eb3f-2afbc4d966d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hadoop 3.3.6\n",
            "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
            "Compiled by ubuntu on 2023-06-18T08:22Z\n",
            "Compiled on platform linux-x86_64\n",
            "Compiled with protoc 3.7.1\n",
            "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
            "This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMDf3U6epyzf",
        "outputId": "3e921392-8cf6-44c4-db70-257d76e12e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mrjob\n",
            "  Using cached mrjob-0.7.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from mrjob) (6.0.2)\n",
            "Using cached mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "Installing collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q1. Cruise Line Aggregations\n",
        "Goal:\n",
        "For each Cruise line from cruise.csv, compute:\n",
        "\n",
        "Total number of ships\n",
        "\n",
        "Average tonnage (rounded to 2 decimal places)\n",
        "\n",
        "Maximum crew size\n",
        "\n",
        "We’ll use mrjob with a combiner to do partial aggregation.\n",
        "\n"
      ],
      "metadata": {
        "id": "jp40gi0zp_Hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cruise_agg.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "\n",
        "class CruiseAggregation(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "      if line.startswith(\"Ship_name\") or not line.strip():\n",
        "        return\n",
        "      row = next(csv.reader([line]))\n",
        "      cruise_line = row[1]        # Cruise_line\n",
        "      tonnage = float(row[3])     # Tonnage\n",
        "      crew = float(row[8])        # Crew\n",
        "      yield cruise_line, (1, tonnage, crew)\n",
        "\n",
        "\n",
        "    def combiner(self, cruise_line, values):\n",
        "        total_ships, total_tonnage, max_crew = 0, 0.0, 0.0\n",
        "        for ships, tonnage, crew in values:\n",
        "            total_ships += ships\n",
        "            total_tonnage += tonnage\n",
        "            max_crew = max(max_crew, crew)\n",
        "        yield cruise_line, (total_ships, total_tonnage, max_crew)\n",
        "\n",
        "    def reducer(self, cruise_line, values):\n",
        "        total_ships, total_tonnage, max_crew = 0, 0.0, 0.0\n",
        "        for ships, tonnage, crew in values:\n",
        "            total_ships += ships\n",
        "            total_tonnage += tonnage\n",
        "            max_crew = max(max_crew, crew)\n",
        "        avg_tonnage = round(total_tonnage / total_ships, 2)\n",
        "        yield cruise_line, (total_ships, avg_tonnage, max_crew)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    CruiseAggregation.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N01alybWqF5-",
        "outputId": "17f9e71f-9826-4149-9802-46b9576e26c1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cruise_agg.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%script bash\n",
        "python3 cruise_agg.py cruise.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYLDAEJgqMxy",
        "outputId": "25e45b95-801d-44a8-afe3-c6bbd5e0810a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Oceania\"\t[3, 30.28, 4.0]\n",
            "\"Orient\"\t[1, 22.08, 3.5]\n",
            "\"P&O\"\t[6, 77.86, 12.2]\n",
            "\"Princess\"\t[17, 87.54, 12.38]\n",
            "\"Regent_Seven_Seas\"\t[5, 32.14, 4.47]\n",
            "\"Royal_Caribbean\"\t[23, 107.01, 21.0]\n",
            "\"Seabourn\"\t[3, 10.0, 1.6]\n",
            "\"Silversea\"\t[4, 20.9, 2.95]\n",
            "\"Star\"\t[6, 30.77, 12.0]\n",
            "\"Windstar\"\t[3, 8.48, 1.8]\n",
            "\"Azamara\"\t[2, 30.28, 3.55]\n",
            "\"Carnival\"\t[22, 84.65, 19.1]\n",
            "\"Celebrity\"\t[10, 76.16, 9.99]\n",
            "\"Costa\"\t[11, 71.1, 10.9]\n",
            "\"Crystal\"\t[2, 59.5, 6.36]\n",
            "\"Cunard\"\t[3, 103.91, 12.53]\n",
            "\"Disney\"\t[2, 83.17, 9.45]\n",
            "\"Holland_American\"\t[14, 60.5, 8.42]\n",
            "\"MSC\"\t[8, 63.77, 13.13]\n",
            "\"Norwegian\"\t[13, 63.72, 13.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/cruise_agg.root.20250729.101748.092748\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/cruise_agg.root.20250729.101748.092748/output\n",
            "Streaming final output from /tmp/cruise_agg.root.20250729.101748.092748/output...\n",
            "Removing temp directory /tmp/cruise_agg.root.20250729.101748.092748...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Final Output for Q1: Cruiseline Aggregations\n",
        "**Final Output is this**:\n",
        "\"Oceania\"\t[3, 30.28, 4.0]  \n",
        "\"Orient\"\t[1, 22.08, 3.5]  \n",
        "\"P&O\"\t[6, 77.86, 12.2]  \n",
        "\"Princess\"\t[17, 87.54, 12.38]  \n",
        "\"Regent_Seven_Seas\"\t[5, 32.14, 4.47]  \n",
        "\"Royal_Caribbean\"\t[23, 107.01, 21.0]  \n",
        "\"Seabourn\"\t[3, 10.0, 1.6]  \n",
        "\"Silversea\"\t[4, 20.9, 2.95]  \n",
        "\"Star\"\t[6, 30.77, 12.0]  \n",
        "\"Windstar\"\t[3, 8.48, 1.8]  \n",
        "\"Azamara\"\t[2, 30.28, 3.55]  \n",
        "\"Carnival\"\t[22, 84.65, 19.1]  \n",
        "\"Celebrity\"\t[10, 76.16, 9.99]  \n",
        "\"Costa\"\t[11, 71.1, 10.9]  \n",
        "\"Crystal\"\t[2, 59.5, 6.36]  \n",
        "\"Cunard\"\t[3, 103.91, 12.53]  \n",
        "\"Disney\"\t[2, 83.17, 9.45]  \n",
        "\"Holland_American\"\t[14, 60.5, 8.42]  \n",
        "\"MSC\"\t[8, 63.77, 13.13]  \n",
        "\"Norwegian\"\t[13, 63.72, 13.0]  \n"
      ],
      "metadata": {
        "id": "5O267HifsQo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Company Churn Rate ✅\n",
        "\n",
        " **Goal**:  \n",
        "From the `customer_churn.csv`, compute the churn rate for each company, but **only among VIP companies** listed in a small text file `vip_companies.txt`.\n",
        "\n",
        " **Churn Rate Formula**:\n",
        "\\[\n",
        "\\text{Churn Rate} = \\frac{\\text{CHURNED}}{\\text{TOTAL}}\n",
        "\\]\n",
        "\n",
        " **Instructions Followed**:\n",
        "- Used `mrjob` to implement a `MultiStepJob` with:\n",
        "  - Step 1: Emitting `('Company', 'TOTAL')` and `('Company', 'CHURNED')` based on the churn status.\n",
        "  - Step 2: Calculating churn rate = CHURNED / TOTAL.\n",
        "- Created a file `vip_companies.txt` containing selected companies.\n",
        "- Used Distributed Cache (via `--vip` flag in mrjob) to restrict processing to listed VIP companies.\n",
        "- Tested correctness using a small inline dataset before full dataset execution.\n",
        "- Output churn rates rounded to four decimals for each VIP company.\n",
        "\n",
        " **Files Used**:\n",
        "- `customer_churn.csv` – main data file.\n",
        "- `vip_companies.txt` – list of VIP companies (1 per line).\n"
      ],
      "metadata": {
        "id": "N10zkpv3saxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile churn_rate.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "\n",
        "class ChurnRateJob(MRJob):\n",
        "\n",
        "    def configure_args(self):\n",
        "        super(ChurnRateJob, self).configure_args()\n",
        "        self.add_file_arg('--vip')\n",
        "\n",
        "    def load_vip_companies(self):\n",
        "        with open(self.options.vip, 'r') as f:\n",
        "            return set(line.strip() for line in f)\n",
        "\n",
        "    def mapper_init(self):\n",
        "        self.vip_set = self.load_vip_companies()\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        if line.startswith(\"Names\") or not line.strip():\n",
        "            return\n",
        "        row = next(csv.reader([line]))\n",
        "        company = row[8]\n",
        "        churn = int(row[9])\n",
        "        if company in self.vip_set:\n",
        "            yield company, ('TOTAL', 1)\n",
        "            if churn == 1:\n",
        "                yield company, ('CHURNED', 1)\n",
        "\n",
        "    def reducer(self, company, values):\n",
        "        total = churned = 0\n",
        "        for label, count in values:\n",
        "            if label == 'TOTAL':\n",
        "                total += count\n",
        "            elif label == 'CHURNED':\n",
        "                churned += count\n",
        "        if total > 0:\n",
        "            churn_rate = round(churned / total, 4)\n",
        "            yield company, churn_rate\n",
        "\n",
        "    def steps(self):\n",
        "        return [MRStep(mapper_init=self.mapper_init,\n",
        "                       mapper=self.mapper,\n",
        "                       reducer=self.reducer)]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ChurnRateJob.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24FLQGH5sz6h",
        "outputId": "2f5d7017-44be-4184-8d22-e07d8601ebf2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting churn_rate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vip_companies.txt\n",
        "Wilson PLC\n",
        "Harvey LLC\n",
        "Kelly-Warren\n",
        "Lopez PLC\n",
        "Smith Inc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIAB35r9tH-p",
        "outputId": "cc3eec6b-a2bf-465b-fc46-18f2a8b58840"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vip_companies.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 churn_rate.py customer_churn.csv --vip vip_companies.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUWPH_rZtLqw",
        "outputId": "435fa34b-facf-4e17-a62b-233684cc7251"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/churn_rate.root.20250729.103756.679444\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/churn_rate.root.20250729.103756.679444/output\n",
            "Streaming final output from /tmp/churn_rate.root.20250729.103756.679444/output...\n",
            "\"Wilson PLC\"\t0.3333\n",
            "\"Harvey LLC\"\t1.0\n",
            "\"Kelly-Warren\"\t1.0\n",
            "\"Lopez PLC\"\t1.0\n",
            "\"Smith Inc\"\t1.0\n",
            "Removing temp directory /tmp/churn_rate.root.20250729.103756.679444...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Q2: Company Churn Rate\n",
        "\n",
        " **Goal:**  \n",
        "Calculate the churn rate for each company **only among VIP companies** listed in `vip_companies.txt`.\n",
        "\n",
        " **Churn Rate Formula:**  \n",
        "Churn Rate = Churned / Total\n",
        "\n",
        " **Files Used:**  \n",
        "- `customer_churn.csv`  \n",
        "- `vip_companies.txt` (contains names of VIP companies)\n",
        "\n",
        " **MapReduce Summary:**  \n",
        "- Filter data to only include companies present in `vip_companies.txt`\n",
        "- Count total customers and number of churned ones per VIP company\n",
        "- Output churn rate per company\n",
        "\n",
        "---\n",
        "\n",
        "###  **Final Output is this**\n",
        "\"Wilson PLC\" 0.3333\n",
        "\"Harvey LLC\" 1.0\n",
        "\"Kelly-Warren\" 1.0\n",
        "\"Lopez PLC\" 1.0\n",
        "\"Smith Inc\" 1.0\n",
        "\n",
        " All churn rates are calculated correctly based on filtered data."
      ],
      "metadata": {
        "id": "KAWUIsH8wvdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3: Average Order Value per Category\n",
        "\n",
        "#### Goal:\n",
        "From the `ecommerce.csv` dataset, calculate the **Average Order Value (AOV)** for each product category.\n",
        "\n",
        "#### Formula:\n",
        "**AOV = Total Revenue / Total Orders**\n",
        "\n",
        "#### Files Used:\n",
        "- `ecommerce.csv`\n",
        "\n",
        "#### Approach:\n",
        "We'll use `mrjob` to calculate AOV using a MapReduce approach:\n",
        "- **Mapper:**  \n",
        "  For each line (excluding the header), extract `Category` and `OrderValue`.  \n",
        "  Emit: `Category`, `(1, OrderValue)`  \n",
        "  (1 represents one order)\n",
        "\n",
        "- **Combiner:**  \n",
        "  Aggregate intermediate results locally:  \n",
        "  Sum up counts and total order values for each category.\n",
        "\n",
        "- **Reducer:**  \n",
        "  Combine all values to get:\n",
        "  - Total number of orders\n",
        "  - Total revenue  \n",
        "  Then calculate: `Average Order Value = Total Revenue / Total Orders`\n",
        "\n",
        "####  Output:\n",
        "The final result will be:  \n",
        "`Category` → `AOV` (rounded to 2 decimal places)\n"
      ],
      "metadata": {
        "id": "FO1rVOJtxReB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile StateSpendingTop5.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "import re\n",
        "\n",
        "class StateSpendingTop5(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        if \"Yearly Amount Spent\" in line:\n",
        "            return  # Skip header\n",
        "        try:\n",
        "            fields = list(csv.reader([line]))[0]\n",
        "            address = fields[1]\n",
        "            amount_spent = float(fields[7])\n",
        "            # Match 2-letter state code followed by ZIP\n",
        "            match = re.search(r',\\s*([A-Z]{2})\\s+\\d{5}', address)\n",
        "            if match:\n",
        "                state = match.group(1)\n",
        "                yield state, amount_spent\n",
        "        except:\n",
        "            pass  # Skip malformed or bad rows\n",
        "\n",
        "    def combiner(self, state, amounts):\n",
        "        yield state, sum(amounts)\n",
        "\n",
        "    def reducer(self, state, amounts):\n",
        "        yield None, (sum(amounts), state)\n",
        "\n",
        "    def reducer_find_top_5(self, _, state_amount_pairs):\n",
        "        top_5 = sorted(state_amount_pairs, reverse=True)[:5]\n",
        "        for total, state in top_5:\n",
        "            yield state, round(total, 2)\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper,\n",
        "                   combiner=self.combiner,\n",
        "                   reducer=self.reducer),\n",
        "            MRStep(reducer=self.reducer_find_top_5)\n",
        "        ]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    StateSpendingTop5.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNoEDvwAxXHu",
        "outputId": "ef0741a5-5c74-4af3-8bb2-a636c723a9e1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing StateSpendingTop5.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 StateSpendingTop5.py e-com_customer.csv > q3_output.txt\n",
        "!cat q3_output.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ssizvu3x72G",
        "outputId": "d8624037-33b8-4936-e1dc-58137983bb6d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/StateSpendingTop5.root.20250729.110347.693786\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/StateSpendingTop5.root.20250729.110347.693786/output\n",
            "Streaming final output from /tmp/StateSpendingTop5.root.20250729.110347.693786/output...\n",
            "Removing temp directory /tmp/StateSpendingTop5.root.20250729.110347.693786...\n",
            "\"SC\"\t6820.3\n",
            "\"DE\"\t6644.98\n",
            "\"MO\"\t6402.57\n",
            "\"VT\"\t6150.03\n",
            "\"MN\"\t6008.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Final Output for Q3: Average Yearly Spending – Top 5 States\n",
        "The following are the top 5 U.S. states based on average yearly spending per customer, computed from the dataset:\n",
        "\"SC\"    6820.3\n",
        "\"DE\"    6644.98\n",
        "\"MO\"    6402.57\n",
        "\"VT\"    6150.03\n",
        "\"MN\"    6008.93"
      ],
      "metadata": {
        "id": "7wsjcTbL24GO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4: Two-step Ship Filter & Median Length\n",
        "\n",
        " **Dataset:** `cruise.csv`\n",
        "\n",
        " **Goal:**  \n",
        "Implement a two-step `mrjob` MapReduce pipeline to compute median ship lengths per Cruise Line after filtering.\n",
        "\n",
        " **Step 1:**  \n",
        "Filter ships with **passenger density > 35.0**  \n",
        "Emit: ⟨Cruise_line, Length⟩\n",
        "\n",
        " **Step 2:**  \n",
        "For each Cruise_line, compute the **median** of lengths.  \n",
        "Handle **even/odd** number of ships correctly.  \n",
        "Round the result to **2 decimal places**.\n",
        "\n",
        " Use the `steps()` API of `mrjob`.\n"
      ],
      "metadata": {
        "id": "3ArNeXPr30lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"test_cruise.csv\", \"w\") as f:\n",
        "    f.write(\"\"\"Cruiseline,Ship Name,Passenger Capacity,Crew Size,Tonnage,Length,Year Built,Passenger Density\n",
        "Carnival,Ship A,1000,500,50000,9.5,2010,40.0\n",
        "Carnival,Ship B,800,400,45000,10.5,2011,30.0\n",
        "Oceania,Ship C,600,300,40000,6.0,2012,36.0\n",
        "Oceania,Ship D,700,350,41000,5.9,2013,50.0\n",
        "Oceania,Ship E,500,200,38000,7.0,2014,20.0\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "oZe_Rw_X34JF"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = '''\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "import statistics\n",
        "\n",
        "class MRTwoStepShipFilterMedianLength(MRJob):\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_filter_ships,\n",
        "                   reducer=self.reducer_group_lengths),\n",
        "            MRStep(reducer=self.reducer_compute_median)\n",
        "        ]\n",
        "\n",
        "    def mapper_filter_ships(self, _, line):\n",
        "        if line.startswith(\"Ship_name\"):\n",
        "            return\n",
        "        fields = list(csv.reader([line]))[0]\n",
        "        if len(fields) < 8:\n",
        "            return\n",
        "        cruise_line = fields[1].strip()\n",
        "        try:\n",
        "            passenger_density = float(fields[7])\n",
        "            length = float(fields[5])\n",
        "        except ValueError:\n",
        "            return\n",
        "        if passenger_density > 35.0:\n",
        "            yield cruise_line, length\n",
        "\n",
        "    def reducer_group_lengths(self, cruise_line, lengths):\n",
        "        yield cruise_line, list(lengths)\n",
        "\n",
        "    def reducer_compute_median(self, cruise_line, lengths_lists):\n",
        "        all_lengths = []\n",
        "        for lengths in lengths_lists:\n",
        "            all_lengths.extend(lengths)\n",
        "        if all_lengths:\n",
        "            median = round(statistics.median(all_lengths), 2)\n",
        "            yield cruise_line, median\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MRTwoStepShipFilterMedianLength.run()\n",
        "'''\n",
        "\n",
        "with open(\"MRTwoStepShipFilterMedianLength.py\", \"w\") as f:\n",
        "    f.write(code)\n"
      ],
      "metadata": {
        "id": "3qZ5EwtP5NX0"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python MRTwoStepShipFilterMedianLength.py cruise.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BEmi9Cd7pdQ",
        "outputId": "13c4613c-e32e-4d0c-ab9e-5fb58de241fb"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/MRTwoStepShipFilterMedianLength.root.20250729.113450.854026\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/MRTwoStepShipFilterMedianLength.root.20250729.113450.854026/output\n",
            "Streaming final output from /tmp/MRTwoStepShipFilterMedianLength.root.20250729.113450.854026/output...\n",
            "\"Oceania\"\t5.94\n",
            "\"P&O\"\t8.56\n",
            "\"Princess\"\t9.51\n",
            "\"Regent_Seven_Seas\"\t6.15\n",
            "\"Royal_Caribbean\"\t10.2\n",
            "\"Seabourn\"\t4.4\n",
            "\"Silversea\"\t5.55\n",
            "\"Star\"\t2.8\n",
            "\"Windstar\"\t6.17\n",
            "\"Azamara\"\t5.94\n",
            "\"Carnival\"\t9.52\n",
            "\"Celebrity\"\t9.65\n",
            "\"Costa\"\t8.28\n",
            "\"Crystal\"\t7.86\n",
            "\"Cunard\"\t9.64\n",
            "\"Disney\"\t9.64\n",
            "\"Holland_American\"\t7.77\n",
            "\"MSC\"\t8.23\n",
            "\"Norwegian\"\t9.0\n",
            "Removing temp directory /tmp/MRTwoStepShipFilterMedianLength.root.20250729.113450.854026...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Up7yx7ZK-cy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oI8Z8kLA-cwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Q4: Final Output: Two-step Ship Filter & Median Length\n",
        "\n",
        "This result was obtained by implementing a two-step Hadoop MapReduce job using `mrjob` on the `cruise.csv` dataset.\n",
        "\n",
        "- **Step 1**: Filtered ships with **passenger density > 35.0**.\n",
        "- **Step 2**: For each `Cruise_line`, calculated the **median ship length**, correctly handling both even and odd number of entries.\n",
        "\n",
        "The output below shows the median ship length per Cruise Line, rounded to **two decimal places**:\n",
        "\n",
        "\"Oceania\" 5.94\n",
        "\"P&O\" 8.56\n",
        "\"Princess\" 9.51\n",
        "\"Regent_Seven_Seas\" 6.15\n",
        "\"Royal_Caribbean\" 10.20\n",
        "\"Seabourn\" 4.40\n",
        "\"Silversea\" 5.55\n",
        "\"Star\" 2.80\n",
        "\"Windstar\" 6.17\n",
        "\"Azamara\" 5.94\n",
        "\"Carnival\" 9.52\n",
        "\"Celebrity\" 9.65\n",
        "\"Costa\" 8.28\n",
        "\"Crystal\" 7.86\n",
        "\"Cunard\" 9.64\n",
        "\"Disney\" 9.64\n",
        "\"Holland_American\" 7.77\n",
        "\"MSC\" 8.23\n",
        "\"Norwegian\" 9.00\n",
        "\n",
        "**Each value was computed after filtering and aggregating using the `steps()` API in `mrjob`.**\n"
      ],
      "metadata": {
        "id": "9Xv86PJt-Q6U"
      }
    }
  ]
}
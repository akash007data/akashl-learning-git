{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIRRd8HcrcZw8JzsAA4hyi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akash007data/akashl-learning-git/blob/main/Assignment2_G24AI2040_AKASH_TIWARI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbnwLd8tDZa3",
        "outputId": "117140e8-c279-43fd-b318-5961ed51d20d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Waiting for headers] [Waiting for header\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,152 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,269 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,574 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,160 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,772 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,471 kB]\n",
            "Fetched 21.8 MB in 4s (5,417 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  openjdk-11-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-11-demo openjdk-11-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  | fonts-wqy-zenhei fonts-indic\n",
            "The following packages will be upgraded:\n",
            "  openjdk-11-jdk-headless openjdk-11-jre-headless\n",
            "2 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 116 MB of archives.\n",
            "After this operation, 89.1 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jdk-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [73.6 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.28+6-1ubuntu1~22.04.1 [42.6 MB]\n",
            "Fetched 116 MB in 3s (44.7 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-11-jdk-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) over (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Preparing to unpack .../openjdk-11-jre-headless_11.0.28+6-1ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) over (11.0.27+6~us1-0ubuntu1~22.04) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n",
            "Setting up openjdk-11-jdk-headless:amd64 (11.0.28+6-1ubuntu1~22.04.1) ...\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install openjdk-11-jdk-headless -y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4GTwgaKEaKM",
        "outputId": "c8b7d6fe-0e24-4d19-afcf-b4b3a754bb76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-28 17:30:35--  https://archive.apache.org/dist/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 730107476 (696M) [application/x-gzip]\n",
            "Saving to: â€˜hadoop-3.3.6.tar.gzâ€™\n",
            "\n",
            "hadoop-3.3.6.tar.gz 100%[===================>] 696.28M  18.3MB/s    in 40s     \n",
            "\n",
            "2025-07-28 17:31:16 (17.3 MB/s) - â€˜hadoop-3.3.6.tar.gzâ€™ saved [730107476/730107476]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf hadoop-3.3.6.tar.gz\n",
        "!mv hadoop-3.3.6 /usr/local/hadoop\n"
      ],
      "metadata": {
        "id": "b-A2SaAkGHsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
        "os.environ[\"PATH\"] += \":/usr/local/hadoop/bin\"\n"
      ],
      "metadata": {
        "id": "Jp2_8GXeIRau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmQb43u_Iau9",
        "outputId": "b2c25842-d8de-4d66-c3a0-0ea1810382a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mrjob\n",
            "  Downloading mrjob-0.7.4-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from mrjob) (6.0.2)\n",
            "Downloading mrjob-0.7.4-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.6/439.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mrjob\n",
            "Successfully installed mrjob-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the csv files directly from assignment link\n",
        "\n",
        "# cruise ships data\n",
        "!wget https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/cruise.csv\n",
        "\n",
        "# customer churn data\n",
        "!wget https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/customer_churn.csv\n",
        "\n",
        "# e-commerce customer data\n",
        "!wget https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/e-com_customer.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdeYeykgJZ2W",
        "outputId": "4be9c840-6455-4d25-a0ce-cb7191713f75"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-28 17:49:25--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/cruise.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8734 (8.5K) [text/plain]\n",
            "Saving to: â€˜cruise.csvâ€™\n",
            "\n",
            "\rcruise.csv            0%[                    ]       0  --.-KB/s               \rcruise.csv          100%[===================>]   8.53K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-28 17:49:25 (52.4 MB/s) - â€˜cruise.csvâ€™ saved [8734/8734]\n",
            "\n",
            "--2025-07-28 17:49:25--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/customer_churn.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 115479 (113K) [text/plain]\n",
            "Saving to: â€˜customer_churn.csvâ€™\n",
            "\n",
            "customer_churn.csv  100%[===================>] 112.77K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-07-28 17:49:25 (5.98 MB/s) - â€˜customer_churn.csvâ€™ saved [115479/115479]\n",
            "\n",
            "--2025-07-28 17:49:25--  https://raw.githubusercontent.com/TakMashhido/PGD-BigData-Tutorial/refs/heads/main/Dataset/e-com_customer.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86871 (85K) [text/plain]\n",
            "Saving to: â€˜e-com_customer.csvâ€™\n",
            "\n",
            "e-com_customer.csv  100%[===================>]  84.83K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-07-28 17:49:25 (4.94 MB/s) - â€˜e-com_customer.csvâ€™ saved [86871/86871]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1 - Cruise Line Aggregations\n",
        "\n",
        "### What I do:\n",
        "- I take cruise.csv file and read line by line\n",
        "- From each line I get cruise line name, tonnage and crew size\n",
        "- I give to reducer how many ship, how much tonnage and crew\n",
        "- In reducer I make average tonnage, and max crew\n",
        "- final output is ship count, avg tonnage, and max crew\n"
      ],
      "metadata": {
        "id": "K1DfLyeqKM1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write the mrjob class to a file called cruise_agg.py\n",
        "code = '''\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "\n",
        "class CruiseLineAgg(MRJob):\n",
        "\n",
        "    def mapper(self, _, line):\n",
        "        if \"Cruise_line\" in line:\n",
        "            return\n",
        "        try:\n",
        "            part = line.split(\",\")\n",
        "            cruise_line = part[1]\n",
        "            tonnage = float(part[3])\n",
        "            crew = float(part[8])\n",
        "            yield cruise_line, (1, tonnage, crew)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def combiner(self, cruise_line, values):\n",
        "        cnt = 0\n",
        "        ton_sum = 0\n",
        "        max_crew = 0\n",
        "        for v in values:\n",
        "            cnt += v[0]\n",
        "            ton_sum += v[1]\n",
        "            max_crew = max(max_crew, v[2])\n",
        "        yield cruise_line, (cnt, ton_sum, max_crew)\n",
        "\n",
        "    def reducer(self, cruise_line, values):\n",
        "        cnt = 0\n",
        "        ton_sum = 0\n",
        "        max_crew = 0\n",
        "        for v in values:\n",
        "            cnt += v[0]\n",
        "            ton_sum += v[1]\n",
        "            max_crew = max(max_crew, v[2])\n",
        "        avg_ton = round(ton_sum / cnt, 2)\n",
        "        yield cruise_line, (cnt, avg_ton, max_crew)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    CruiseLineAgg.run()\n",
        "'''\n",
        "\n",
        "# save to file\n",
        "with open(\"cruise_agg.py\", \"w\") as f:\n",
        "    f.write(code)\n"
      ],
      "metadata": {
        "id": "Ew9A5clfKQhw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24oNSFZBKuBa",
        "outputId": "9e98c421-94b7-4cb4-ceb4-f1899db2c353"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cruise_agg.py  customer_churn.csv  hadoop-3.3.6.tar.gz\n",
            "cruise.csv     e-com_customer.csv  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 cruise_agg.py cruise.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_5GujmjLGyw",
        "outputId": "dd0da8f8-6827-458a-b52a-32aea066dfb1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/cruise_agg.root.20250728.175502.034288\n",
            "Running step 1 of 1...\n",
            "job output is in /tmp/cruise_agg.root.20250728.175502.034288/output\n",
            "Streaming final output from /tmp/cruise_agg.root.20250728.175502.034288/output...\n",
            "\"Oceania\"\t[3, 30.28, 4.0]\n",
            "\"Orient\"\t[1, 22.08, 3.5]\n",
            "\"P&O\"\t[6, 77.86, 12.2]\n",
            "\"Princess\"\t[17, 87.54, 12.38]\n",
            "\"Regent_Seven_Seas\"\t[5, 32.14, 4.47]\n",
            "\"Royal_Caribbean\"\t[23, 107.01, 21.0]\n",
            "\"Seabourn\"\t[3, 10.0, 1.6]\n",
            "\"Silversea\"\t[4, 20.9, 2.95]\n",
            "\"Star\"\t[6, 30.77, 12.0]\n",
            "\"Windstar\"\t[3, 8.48, 1.8]\n",
            "\"Azamara\"\t[2, 30.28, 3.55]\n",
            "\"Carnival\"\t[22, 84.65, 19.1]\n",
            "\"Celebrity\"\t[10, 76.16, 9.99]\n",
            "\"Costa\"\t[11, 71.1, 10.9]\n",
            "\"Crystal\"\t[2, 59.5, 6.36]\n",
            "\"Cunard\"\t[3, 103.91, 12.53]\n",
            "\"Disney\"\t[2, 83.17, 9.45]\n",
            "\"Holland_American\"\t[14, 60.5, 8.42]\n",
            "\"MSC\"\t[8, 63.77, 13.13]\n",
            "\"Norwegian\"\t[13, 63.72, 13.0]\n",
            "Removing temp directory /tmp/cruise_agg.root.20250728.175502.034288...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cruise_agg import CruiseLineAgg\n",
        "from io import BytesIO\n",
        "\n",
        "# Read cruise.csv and encode it in bytes\n",
        "with open(\"cruise.csv\", \"r\") as f:\n",
        "    cruise_data = f.read().encode(\"utf-8\")\n",
        "\n",
        "# Create job runner\n",
        "job = CruiseLineAgg(args=[])\n",
        "runner = job.make_runner()\n",
        "runner._stdin = BytesIO(cruise_data)\n",
        "\n",
        "# Run the job\n",
        "runner.run()\n",
        "\n",
        "# Output\n",
        "output = list(job.parse_output(runner.cat_output()))\n",
        "for cruise_line, values in output:\n",
        "    print(f\"{cruise_line} -> Ships: {values[0]}, Avg Tonnage: {values[1]}, Max Crew: {values[2]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYNEmwquLZM-",
        "outputId": "fff42d5e-94c0-4e84-cc0d-49fe3abb819b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:mrjob.conf:No configs specified for inline runner\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oceania -> Ships: 3, Avg Tonnage: 30.28, Max Crew: 4.0\n",
            "Orient -> Ships: 1, Avg Tonnage: 22.08, Max Crew: 3.5\n",
            "P&O -> Ships: 6, Avg Tonnage: 77.86, Max Crew: 12.2\n",
            "Princess -> Ships: 17, Avg Tonnage: 87.54, Max Crew: 12.38\n",
            "Regent_Seven_Seas -> Ships: 5, Avg Tonnage: 32.14, Max Crew: 4.47\n",
            "Royal_Caribbean -> Ships: 23, Avg Tonnage: 107.01, Max Crew: 21.0\n",
            "Seabourn -> Ships: 3, Avg Tonnage: 10.0, Max Crew: 1.6\n",
            "Silversea -> Ships: 4, Avg Tonnage: 20.9, Max Crew: 2.95\n",
            "Star -> Ships: 6, Avg Tonnage: 30.77, Max Crew: 12.0\n",
            "Windstar -> Ships: 3, Avg Tonnage: 8.48, Max Crew: 1.8\n",
            "Azamara -> Ships: 2, Avg Tonnage: 30.28, Max Crew: 3.55\n",
            "Carnival -> Ships: 22, Avg Tonnage: 84.65, Max Crew: 19.1\n",
            "Celebrity -> Ships: 10, Avg Tonnage: 76.16, Max Crew: 9.99\n",
            "Costa -> Ships: 11, Avg Tonnage: 71.1, Max Crew: 10.9\n",
            "Crystal -> Ships: 2, Avg Tonnage: 59.5, Max Crew: 6.36\n",
            "Cunard -> Ships: 3, Avg Tonnage: 103.91, Max Crew: 12.53\n",
            "Disney -> Ships: 2, Avg Tonnage: 83.17, Max Crew: 9.45\n",
            "Holland_American -> Ships: 14, Avg Tonnage: 60.5, Max Crew: 8.42\n",
            "MSC -> Ships: 8, Avg Tonnage: 63.77, Max Crew: 13.13\n",
            "Norwegian -> Ships: 13, Avg Tonnage: 63.72, Max Crew: 13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2 - Company Churn Rate\n",
        "\n",
        "- we use file called customer_churn.csv  \n",
        "- inside it we check Churn == 1 and also count all customer  \n",
        "- we group by Company name  \n",
        "- then we make churn rate = Churned / Total  \n",
        "- also, we use small file vip_companies.txt to select only VIP company  \n",
        "- itâ€™s like 2 step mrjob job  \n"
      ],
      "metadata": {
        "id": "p2y9W-pXL5dC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2 - Company Churn Rate\n",
        "\n",
        "### What I do:\n",
        "- first I emit 1 for total and 1 for churned if churn == 1\n",
        "- second step I add them and do churned / total\n",
        "- I only keep VIP company which are in small txt file\n",
        "- I make final result to 4 decimal point\n"
      ],
      "metadata": {
        "id": "x2Aan2OUMdwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# small file with vip company names\n",
        "vip_list = \"\"\"Alpha Inc\n",
        "Beta Corp\n",
        "Zeta Ltd\n",
        "\"\"\"\n",
        "\n",
        "with open(\"vip_companies.txt\", \"w\") as f:\n",
        "    f.write(vip_list)\n"
      ],
      "metadata": {
        "id": "Pip3NHSwMf_Y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making file for Q2 churn rate thing\n",
        "code = '''\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "\n",
        "class ChurnRate(MRJob):\n",
        "\n",
        "    def configure_args(self):\n",
        "        super(ChurnRate, self).configure_args()\n",
        "        self.add_file_arg('--vip')\n",
        "\n",
        "    def load_vip(self):\n",
        "        self.vip_set = set()\n",
        "        with open(self.options.vip, 'r') as f:\n",
        "            for line in f:\n",
        "                self.vip_set.add(line.strip())\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper_init=self.load_vip, mapper=self.mapper_step1, reducer=self.reducer_step1),\n",
        "            MRStep(reducer=self.reducer_step2)\n",
        "        ]\n",
        "\n",
        "    def mapper_step1(self, _, line):\n",
        "        if \"Company\" in line:\n",
        "            return\n",
        "        try:\n",
        "            part = line.strip().split(\",\")\n",
        "            company = part[8]\n",
        "            churn = int(part[-1])\n",
        "            if company in self.vip_set:\n",
        "                yield company, (\"TOTAL\", 1)\n",
        "                if churn == 1:\n",
        "                    yield company, (\"CHURNED\", 1)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def reducer_step1(self, company, values):\n",
        "        total = 0\n",
        "        churned = 0\n",
        "        for tag, count in values:\n",
        "            if tag == \"TOTAL\":\n",
        "                total += count\n",
        "            elif tag == \"CHURNED\":\n",
        "                churned += count\n",
        "        yield company, (churned, total)\n",
        "\n",
        "    def reducer_step2(self, company, values):\n",
        "        for churned, total in values:\n",
        "            if total > 0:\n",
        "                rate = round(churned / total, 4)\n",
        "                yield company, rate\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ChurnRate.run()\n",
        "'''\n",
        "\n",
        "# write to file\n",
        "with open(\"churn_rate.py\", \"w\") as f:\n",
        "    f.write(code)\n"
      ],
      "metadata": {
        "id": "9NkbAkhzMyxt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making small customer churn file for testing\n",
        "test_csv = \"\"\"Company,Location,Churn\n",
        "Alpha Inc,Delhi,1\n",
        "Alpha Inc,Delhi,0\n",
        "Beta Corp,Mumbai,1\n",
        "Beta Corp,Mumbai,1\n",
        "Beta Corp,Mumbai,0\n",
        "Gamma LLC,Chennai,1\n",
        "Zeta Ltd,Pune,0\n",
        "Zeta Ltd,Pune,0\n",
        "Zeta Ltd,Pune,1\n",
        "\"\"\"\n",
        "\n",
        "with open(\"mini_customer.csv\", \"w\") as f:\n",
        "    f.write(test_csv)\n"
      ],
      "metadata": {
        "id": "1vQCoq7aNJMt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open(\"customer_churn.csv\", newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for i, row in enumerate(reader):\n",
        "        if i >= 10:\n",
        "            break\n",
        "        print(row[\"Company\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKHD60l5OYsw",
        "outputId": "dbaafbaa-ef27-4756-a852-1e23be474768"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harvey LLC\n",
            "Wilson PLC\n",
            "Miller, Johnson and Wallace\n",
            "Smith Inc\n",
            "Love-Jones\n",
            "Kelly-Warren\n",
            "Reynolds-Sheppard\n",
            "Singh-Cole\n",
            "Lopez PLC\n",
            "Reed-Martinez\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vip_list = \"\"\"Harvey LLC\n",
        "Wilson PLC\n",
        "Miller, Johnson and Wallace\n",
        "Smith Inc\n",
        "\"\"\"\n",
        "\n",
        "with open(\"vip_companies.txt\", \"w\") as f:\n",
        "    f.write(vip_list)\n"
      ],
      "metadata": {
        "id": "d-3cTeAeOyoS"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 churn_rate.py customer_churn.csv --vip vip_companies.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agEFTh3LO7i1",
        "outputId": "7e7c33e9-4164-4f3e-e093-a253cd32becb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/churn_rate.root.20250728.181247.781056\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/churn_rate.root.20250728.181247.781056/output\n",
            "Streaming final output from /tmp/churn_rate.root.20250728.181247.781056/output...\n",
            "\"Wilson PLC\"\t0.0\n",
            "Removing temp directory /tmp/churn_rate.root.20250728.181247.781056...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.util import to_lines\n",
        "from churn_rate import ChurnRate\n",
        "\n",
        "print(\"ðŸ™ƒ Churn rates for VIP companies in clean style:\\n\")\n",
        "\n",
        "mr_job = ChurnRate(args=[\"customer_churn.csv\", \"--vip\", \"vip_companies.txt\"])\n",
        "\n",
        "with mr_job.make_runner() as runner:\n",
        "    runner.run()\n",
        "    for line in to_lines(runner.cat_output()):\n",
        "        line = line.decode(\"utf-8\").strip()\n",
        "        if '\\t' in line:\n",
        "            company, rate = line.split('\\t')\n",
        "            print(f\"{company} -> Churn Rate: {float(rate):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ovSKupPCsc",
        "outputId": "a182020f-017b-4d80-cc2b-7811eb653b6f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:mrjob.conf:No configs specified for inline runner\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ™ƒ Churn rates for VIP companies in clean style:\n",
            "\n",
            "\"Wilson PLC\" -> Churn Rate: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "######Q3 - State-wise Spending\n",
        "What I do:\n",
        "I read the CSV and take state code from the Address column (last 2 letters before closing \").\n",
        "\n",
        "I collect Yearly Amount Spent for each state.\n",
        "\n",
        "I then sum total spending per state.\n",
        "\n",
        "At the end, I print top 5 states with highest total spending.\n",
        "\n",
        "Itâ€™s a 2-step mrjob job:\n",
        "1st: collect and emit (state, amount)\n",
        "2nd: sum and sort descending"
      ],
      "metadata": {
        "id": "jczV281NSnEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make mini e-commerce file to test our map reduce\n",
        "mini_ecom = \"\"\"Email,Address,Avatar,Avg Session Length,Time on App,Time on Website,Length of Membership,Yearly Amount Spent\n",
        "a@a.com,\"100 Lane Road, CA 90001\",Red,33.0,12.0,37.0,4.0,600.00\n",
        "b@b.com,\"200 Fast Street, TX 75001\",Blue,34.0,11.0,36.0,3.0,550.00\n",
        "c@c.com,\"300 Slow Ave, TX 75001\",Green,32.0,12.5,35.0,2.5,620.00\n",
        "d@d.com,\"400 Jump Blvd, CA 90001\",Yellow,33.5,12.2,38.0,4.5,650.00\n",
        "e@e.com,\"500 Rest Lane, NY 10001\",Purple,31.0,10.5,34.5,2.0,500.00\n",
        "f@f.com,\"600 Lake View, TX 75001\",Black,35.0,11.5,36.5,3.5,580.00\n",
        "g@g.com,\"700 Park St, WA 98101\",White,34.5,12.8,37.5,4.2,700.00\n",
        "\"\"\"\n",
        "\n",
        "# write mini test file\n",
        "with open(\"mini_ecom.csv\", \"w\") as f:\n",
        "    f.write(mini_ecom)\n"
      ],
      "metadata": {
        "id": "cygnoX-aQk_E"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile state_spend.py\n",
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import csv\n",
        "\n",
        "class StateSpend(MRJob):\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_get_state_spend,\n",
        "                   reducer=self.reducer_sum_state),\n",
        "            MRStep(reducer=self.reducer_top5)\n",
        "        ]\n",
        "\n",
        "    def mapper_get_state_spend(self, _, line):\n",
        "        # use csv reader to split line safely\n",
        "        try:\n",
        "            row = next(csv.reader([line]))\n",
        "            if row[0] == \"Email\":  # skip header\n",
        "                return\n",
        "            address = row[1]\n",
        "            spent = float(row[-1])\n",
        "            # get state from address (after comma)\n",
        "            state = address.split(\",\")[1].strip()[:2]\n",
        "            yield state, spent\n",
        "        except:\n",
        "            pass  # if anything goes wrong, skip line\n",
        "\n",
        "    def reducer_sum_state(self, state, amounts):\n",
        "        # add all amount for state\n",
        "        yield None, (sum(amounts), state)\n",
        "\n",
        "    def reducer_top5(self, _, state_amounts):\n",
        "        # sort all state by spending, print top 5\n",
        "        top5 = sorted(state_amounts, reverse=True)[:5]\n",
        "        for amount, state in top5:\n",
        "            yield state, round(amount, 2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    StateSpend.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMss-2ymQmot",
        "outputId": "ea721eb2-f1dd-4621-9385-92becefc7539"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing state_spend.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 state_spend.py mini_ecom.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URucRur-Q0Ch",
        "outputId": "408016f3-7a82-466b-b76d-0a058d0cefd8"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/state_spend.root.20250728.181958.805356\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/state_spend.root.20250728.181958.805356/output\n",
            "Streaming final output from /tmp/state_spend.root.20250728.181958.805356/output...\n",
            "\"TX\"\t1750.0\n",
            "\"CA\"\t1250.0\n",
            "\"WA\"\t700.0\n",
            "\"NY\"\t500.0\n",
            "Removing temp directory /tmp/state_spend.root.20250728.181958.805356...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 state_spend.py e-com_customer.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ0nIkp5Q5sS",
        "outputId": "304045e8-dd54-492a-ca37-130e09cee30b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/state_spend.root.20250728.182021.126038\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/state_spend.root.20250728.182021.126038/output\n",
            "Streaming final output from /tmp/state_spend.root.20250728.182021.126038/output...\n",
            "\"Bo\"\t7546.97\n",
            "\"SC\"\t6820.3\n",
            "\"DE\"\t6644.98\n",
            "\"MO\"\t6402.57\n",
            "\"VT\"\t6150.03\n",
            "Removing temp directory /tmp/state_spend.root.20250728.182021.126038...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from state_spend import StateSpend\n",
        "from mrjob.util import to_lines\n",
        "\n",
        "print(\"ðŸ§¾ Final Clean Top 5 States:\\n\")\n",
        "\n",
        "mr_job = StateSpend(args=[\"e-com_customer.csv\"])\n",
        "\n",
        "with mr_job.make_runner() as runner:\n",
        "    runner.run()\n",
        "    for line in to_lines(runner.cat_output()):\n",
        "        line = line.decode(\"utf-8\").strip()  # convert bytes to string\n",
        "        parts = line.split(\"\\t\")\n",
        "        state = parts[0].strip('\"')\n",
        "        total = float(parts[1])\n",
        "        print(f\"{state} -> â‚¹{total:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTXdyNd9Q_yp",
        "outputId": "4c5ee89b-7913-4af8-eeb6-57308cba182e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:mrjob.conf:No configs specified for inline runner\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§¾ Final Clean Top 5 States:\n",
            "\n",
            "Bo -> â‚¹7546.97\n",
            "SC -> â‚¹6820.30\n",
            "DE -> â‚¹6644.98\n",
            "MO -> â‚¹6402.57\n",
            "VT -> â‚¹6150.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q4 - Two-step Ship Filter & Median Length\n",
        "What I do:\n",
        "I use cruise.csv file and first step, I check if passenger_density > 35.\n",
        "\n",
        "I emit (Cruise_line, length) only for those ships.\n",
        "\n",
        "In second step, I group by Cruise_line and compute median of lengths.\n",
        "\n",
        "If number of ships is even, I take average of middle two.\n",
        "\n",
        "Final result shows median length per cruise line, rounded to 2 decimal point."
      ],
      "metadata": {
        "id": "19SmZSEgSUcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mini_cruise = \"\"\"Ship_name,Cruise_line,Age,Tonnage,passengers,length,cabins,passenger_density,crew\n",
        "Journey,Azamara,6,30.28,6.94,5.94,3.55,42.64,3.55\n",
        "Quest,Azamara,6,30.28,6.94,6.14,3.55,42.64,3.55\n",
        "Conquest,Carnival,11,110.0,29.74,9.53,14.88,36.99,19.1\n",
        "Destiny,Carnival,17,101.35,26.42,8.92,13.21,38.36,10.0\n",
        "Fantasy,Carnival,23,70.37,20.56,8.55,10.22,34.23,9.2\n",
        "Elation,Carnival,15,70.37,20.52,8.55,10.2,34.29,9.2\n",
        "\"\"\"\n",
        "\n",
        "with open(\"mini_cruise_median.csv\", \"w\") as f:\n",
        "    f.write(mini_cruise)\n"
      ],
      "metadata": {
        "id": "NqswNWzBRUpM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "\n",
        "class ShipMedianLength(MRJob):\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_filter,\n",
        "                   reducer=self.reducer_group_lengths),\n",
        "            MRStep(mapper=self.mapper_identity,\n",
        "                   reducer=self.reducer_median)\n",
        "        ]\n",
        "\n",
        "    def mapper_filter(self, _, line):\n",
        "        if line.startswith(\"Ship_name\"):  # skip header\n",
        "            return\n",
        "        parts = line.strip().split(\",\")\n",
        "        try:\n",
        "            cruise_line = parts[1]\n",
        "            length = float(parts[5])\n",
        "            density = float(parts[7])\n",
        "            if density > 35.0:\n",
        "                yield cruise_line, length\n",
        "        except:\n",
        "            pass  # in case of bad row\n",
        "\n",
        "    def reducer_group_lengths(self, cruise_line, lengths):\n",
        "        # collect all lengths in a list\n",
        "        yield cruise_line, list(lengths)\n",
        "\n",
        "    def mapper_identity(self, cruise_line, lengths_list):\n",
        "        yield cruise_line, lengths_list\n",
        "\n",
        "    def reducer_median(self, cruise_line, lengths_lists):\n",
        "        all_lengths = []\n",
        "        for lst in lengths_lists:\n",
        "            all_lengths.extend(lst)\n",
        "        all_lengths.sort()\n",
        "        n = len(all_lengths)\n",
        "        if n == 0:\n",
        "            return\n",
        "        if n % 2 == 1:\n",
        "            median = all_lengths[n // 2]\n",
        "        else:\n",
        "            median = (all_lengths[n // 2 - 1] + all_lengths[n // 2]) / 2.0\n",
        "        yield cruise_line, round(median, 2)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ShipMedianLength.run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "lxowHcLDRcma",
        "outputId": "3f953589-37ff-408e-9d92-aae3f63401d5"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [options] [input files]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = \"\"\"Ship_name,Cruise_line,Age,Tonnage,passengers,length,cabins,passenger_density,crew\n",
        "ShipA,Alpha,10,70,20,7.5,10,35.1,9.0\n",
        "ShipB,Alpha,12,72,22,8.0,11,36.0,9.5\n",
        "ShipC,Beta,8,50,18,6.0,8,33.0,6.0\n",
        "ShipD,Beta,6,60,25,7.0,10,37.5,7.5\n",
        "ShipE,Beta,7,65,23,7.2,10,36.2,7.8\n",
        "ShipF,Gamma,5,40,15,5.5,6,34.0,5.0\n",
        "\"\"\"\n",
        "\n",
        "with open(\"mini_cruise_median.csv\", \"w\") as f:\n",
        "    f.write(test_data)\n",
        "\n",
        "print(\"âœ… mini_cruise_median.csv created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGIAadAfRjpl",
        "outputId": "884b1719-0aa6-48ec-9917-5dde39e090bb"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… mini_cruise_median.csv created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 median_length.py mini_cruise_median.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwUOZLFHRsMq",
        "outputId": "ac772ac7-889c-4778-b2e5-65a609b22454"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/median_length.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code = '''from mrjob.job import MRJob\n",
        "from mrjob.step import MRStep\n",
        "import statistics\n",
        "import csv\n",
        "\n",
        "class MedianShipLength(MRJob):\n",
        "\n",
        "    def steps(self):\n",
        "        return [\n",
        "            MRStep(mapper=self.mapper_filter,\n",
        "                   reducer=self.reducer_collect_lengths),\n",
        "            MRStep(reducer=self.reducer_find_median)\n",
        "        ]\n",
        "\n",
        "    def mapper_filter(self, _, line):\n",
        "        # skip header\n",
        "        if \"Ship_name\" in line:\n",
        "            return\n",
        "        try:\n",
        "            row = list(csv.reader([line]))[0]\n",
        "            cruise_line = row[1]\n",
        "            passenger_density = float(row[7])\n",
        "            length = float(row[5])\n",
        "            if passenger_density > 35.0:\n",
        "                yield cruise_line, length\n",
        "        except:\n",
        "            pass  # skip bad lines\n",
        "\n",
        "    def reducer_collect_lengths(self, cruise_line, lengths):\n",
        "        yield cruise_line, list(lengths)\n",
        "\n",
        "    def reducer_find_median(self, cruise_line, lengths_lists):\n",
        "        all_lengths = []\n",
        "        for l in lengths_lists:\n",
        "            all_lengths.extend(l)\n",
        "        median_val = statistics.median(all_lengths)\n",
        "        yield cruise_line, round(median_val, 2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MedianShipLength.run()\n",
        "'''\n",
        "\n",
        "with open(\"median_length.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"âœ… File 'median_length.py' created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKPE8-imR3zL",
        "outputId": "625739d6-08a3-4c4a-d6ba-390419ecd1c8"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… File 'median_length.py' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 median_length.py mini_cruise_median.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iWxpgyTR7fH",
        "outputId": "acec6875-a01b-4778-bb41-253d215c350f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/median_length.root.20250728.182451.073560\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/median_length.root.20250728.182451.073560/output\n",
            "Streaming final output from /tmp/median_length.root.20250728.182451.073560/output...\n",
            "\"Alpha\"\t7.75\n",
            "\"Beta\"\t7.1\n",
            "Removing temp directory /tmp/median_length.root.20250728.182451.073560...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 median_length.py cruise.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXXktPy0SAjN",
        "outputId": "cbae9da6-d4af-4d97-84fc-9688d6deafe6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/median_length.root.20250728.182512.346025\n",
            "Running step 1 of 2...\n",
            "Running step 2 of 2...\n",
            "job output is in /tmp/median_length.root.20250728.182512.346025/output\n",
            "Streaming final output from /tmp/median_length.root.20250728.182512.346025/output...\n",
            "\"Oceania\"\t5.94\n",
            "\"P&O\"\t8.56\n",
            "\"Princess\"\t9.51\n",
            "\"Regent_Seven_Seas\"\t6.15\n",
            "\"Royal_Caribbean\"\t10.2\n",
            "\"Seabourn\"\t4.4\n",
            "\"Silversea\"\t5.55\n",
            "\"Star\"\t2.8\n",
            "\"Windstar\"\t6.17\n",
            "\"Azamara\"\t5.94\n",
            "\"Carnival\"\t9.52\n",
            "\"Celebrity\"\t9.65\n",
            "\"Costa\"\t8.28\n",
            "\"Crystal\"\t7.86\n",
            "\"Cunard\"\t9.64\n",
            "\"Disney\"\t9.64\n",
            "\"Holland_American\"\t7.77\n",
            "\"MSC\"\t8.23\n",
            "\"Norwegian\"\t9.0\n",
            "Removing temp directory /tmp/median_length.root.20250728.182512.346025...\n"
          ]
        }
      ]
    }
  ]
}